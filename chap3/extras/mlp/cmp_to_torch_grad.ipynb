{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_w3         | exact: False | approximate: True  | maxdiff: 4.470348358154297e-08\n",
      "grad_b3         | exact: False | approximate: True  | maxdiff: 9.546056389808655e-09\n",
      "grad_w2         | exact: False | approximate: True  | maxdiff: 2.2351741790771484e-08\n",
      "grad_b2         | exact: False | approximate: True  | maxdiff: 1.1175870895385742e-08\n",
      "grad_w1         | exact: False | approximate: True  | maxdiff: 8.940696716308594e-08\n",
      "grad_w1         | exact: False | approximate: True  | maxdiff: 8.940696716308594e-08\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-2 * np.pi, 2 * np.pi, 1000)\n",
    "y = np.cos(x)\n",
    "\n",
    "x_train = torch.tensor(x, dtype=torch.float32).unsqueeze(1)\n",
    "y_train = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "class SimpleMLP:\n",
    "    def __init__(self):\n",
    "        self.w1 = torch.randn(1, 100, dtype=torch.float32) * 0.1\n",
    "        self.w1.requires_grad_()\n",
    "        self.b1 = torch.zeros(100, dtype=torch.float32)\n",
    "        self.b1.requires_grad_()\n",
    "        self.w2 = torch.randn(100, 100, dtype=torch.float32) * 0.1\n",
    "        self.w2.requires_grad_()\n",
    "        self.b2 = torch.zeros(100, dtype=torch.float32)\n",
    "        self.b2.requires_grad_()\n",
    "        self.w3 = torch.randn(100, 1, dtype=torch.float32) * 0.1\n",
    "        self.w3.requires_grad_()\n",
    "        self.b3 = torch.zeros(1, dtype=torch.float32)\n",
    "        self.b3.requires_grad_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = torch.matmul(x, self.w1) + self.b1\n",
    "        self.a1 = torch.tanh(self.z1)\n",
    "        self.z2 = torch.matmul(self.a1, self.w2) + self.b2\n",
    "        self.a2 = torch.tanh(self.z2)\n",
    "        self.z3 = torch.matmul(self.a2, self.w3) + self.b3\n",
    "        return self.z3\n",
    "\n",
    "model = SimpleMLP()\n",
    "\n",
    "def cmp(s, dt, t):\n",
    "  ex = torch.all(dt == t.grad).item()\n",
    "  app = torch.allclose(dt, t.grad)\n",
    "  maxdiff = (dt - t.grad).abs().max().item()\n",
    "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
    "\n",
    "y_pred = model.forward(x_train)\n",
    "loss = ((y_pred - y_train) ** 2).mean()\n",
    "loss.backward()\n",
    "\n",
    "grad_z3 = 2 * (y_pred - y_train) / y_train.size(0)\n",
    "grad_w3 = torch.matmul(model.a2.t(), grad_z3)\n",
    "cmp('grad_w3', grad_w3, model.w3)\n",
    "grad_b3 = grad_z3.sum(0)\n",
    "cmp('grad_b3', grad_b3, model.b3)\n",
    "\n",
    "grad_a2 = torch.matmul(grad_z3, model.w3.t())\n",
    "grad_z2 = grad_a2 * (1 - model.a2 ** 2)\n",
    "grad_w2 = torch.matmul(model.a1.t(), grad_z2)\n",
    "cmp('grad_w2', grad_w2, model.w2)\n",
    "\n",
    "grad_b2 = grad_z2.sum(0)\n",
    "cmp('grad_b2', grad_b2, model.b2)\n",
    "\n",
    "\n",
    "grad_a1 = torch.matmul(grad_z2, model.w2.t())\n",
    "grad_z1 = grad_a1 * (1 - model.a1 ** 2)\n",
    "grad_w1 = torch.matmul(x_train.t(), grad_z1)\n",
    "cmp('grad_w1', grad_w1, model.w1)\n",
    "\n",
    "grad_b1 = grad_z1.sum(0)\n",
    "cmp('grad_w1', grad_w1, model.w1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
